<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
		<!-- Global Site Tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=UA-79453179-2"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments)};
		  gtag('js', new Date());

		  gtag('config', 'UA-79453179-2');
		</script>
		<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" id="MathJax-script"></script>

		<title>Workshop on Functional Inference and Machine Intelligence 2023</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
	</head>
	<body>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header" class="alt">
						<!-- <span class="logo"><img src="images/logo.svg" alt="" /></span> -->
						<h1 style="color:white">Workshop on Functional Inference and Machine Intelligence</h1>
						<h3 style="color:white">Tokyo/online (Hybrid), March 14-16, 2023.</h3>

					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul>
							<li><a href="#home" class="active">Home</a></li>
							<li><a href="#program">Program</a></li>
<!--							<li><a href="#poster">Poster Session</a></li>-->
<!--							<li><a href="#schedule">Schedule</a></li>-->
							<li><a href="#organizers">Organizers</a></li>
							<li><a href="#location">Access</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Introduction -->
							<section id="home" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Home</h2>
										</header>
										<p>The <strong>Workshop on Functional Inference and Machine Intelligence (FIMI)</strong> is an international workshop on machine learning and statistics, with a particular focus on theory, methods, and practice. It consists of invited talks, and poster sessions are also planned.  The topics include (but not limited to):</p>
										<ul>
											<li>Machine Learning Methods</li>
											<li>Deep Learning</li>
											<li>Kernel Methods</li>
											<li>Probabilistic Methods</li>
										</ul>
										<p>The workshop will be hybrid. All schedules are in Japan Standard Time (GMT+9).</p>
										<!--Access information to the online platform will be available a week before the workshop date.-->
										
										


										<div Align="center">								
										<p> Registration for in-person meeting (registration will be closed once the number reaches the maximum capacity 60). </p>
									<a href="https://forms.gle/Wrc3xaHx8DPE2F179" 
										target="_blank" class="square_btn">
											<span>Registration (in person)</span>
										</a>
									<p> Registration for online participation (Zoom link will be emailed).</p>
									<a href="https://forms.gle/x5yLdbJy9MsGUa856" 
										target="_blank" class="square_btn">
											<span>Registration (online)</span>
										</a>
										</div>
										
										
										<div Align="center">
											
										<p>Previous Workshop: 
											<a href="https://sites.google.com/site/2016pgm/" target="_blank">2016</a>, 
											<a href="https://sites.google.com/site/2017pgm/home" target="_blank">2017</a>, 
											<a href="https://ismseminar.github.io/fimi2018/" target="_blank">2018</a>, 
											<a href="https://ismseminar.github.io/fimi2019/" target="_blank">2019</a>,
											<a href="https://ismseminar.github.io/fimi2020/" target="_blank">2020</a>,
											<a href="https://ismseminar.github.io/fimi2021/" target="_blank">2021</a></br>
											<a href="https://ismseminar.github.io/fimi2022/" target="_blank">2022</a></br>
											<img src='images/IMG_1213.JPG' alt='' width="40%" align="middle" hspace="5%"></p>
										</div>
									</div>
								</div>

<!--
							<div class="content">
								<header class="major">
									<h2>Invited Speakers</h2>
								</header>
							<ul class="alt2">
							<li>
							<a href="https://www.gatsby.ucl.ac.uk/~gretton/" target="_blank"><b>Arthur Gretton</b></a> (University College London)</br>
							</li>
							<li>
							<a href="https://sites.google.com/view/sophielanger/start" target="_blank"><b>Sophie Langer</b></a> (Technical University of Darmstadt)</br>
							</li>
							<li>
							<a href="https://sites.google.com/view/shosonoda/home" target="_blank"><b>Sho Sonoda</b></a> (RIKEN Advanced Intelligence Project)</br>
							</li>
							<li>
							<a href="https://yutomiyatake.github.io" target="_blank"><b>Yuto Miyatake</b></a> (Osaka University)</br>
							</li>
							<li>
							<a href="http://ibis.t.u-tokyo.ac.jp/suzuki/" target="_blank"><b>Taiji Suzuki</b></a> (The University of Tokyo)</br>
							</li>
							<li>
							<a href="https://www.ism.ac.jp/~fukumizu/" target="_blank"><b>Kenji Fukumizu</b></a> (The Institute of Statistical Mathematics)</br>
							</li>
							<li>
							<a href="https://sites.google.com/view/mimaizumi/home" target="_blank"><b>Masaaki Imaizumi</b></a> (The University of Tokyo)</br>
							</li>
							<li>
							<a href="https://www.microsoft.com/en-us/research/people/gregyang/" target="_blank"><b>Greg Yang</b></a> (Microsoft Research)</br>
							</li>
							<li>
							<a href="https://allmodelsarewrong.net" target="_blank"><b>Song Liu</b></a> (University of Bristol)</br>
							</li>
							<li>
							<a href="https://hermite.jp" target="_blank"><b>Han Bao</b></a> (Kyoto University)</br>
							</li>
							<li>
							<a href="https://ai.stanford.edu/~tengyuma/" target="_blank"><b>Tengyu Ma</b></a> (Stanford University)</br>
							</li>
							<li>
							<a href="https://sejdino.github.io" target="_blank"><b>Dino Sejdinovic</b></a> (University of Adelaide)</br>
							</li>
							<li>
							<a href="https://kaba-lab.org/en/" target="_blank"><b>Yoshiyuki Kabashima</b></a> (The University of Tokyo)</br>
							</li>
							<li>
							<a href="https://takaosa.github.io/index.html" target="_blank"><b>Takayuki Osa</b></a> (The University of Tokyo)</br>
							</li>
							<li>
							<a href="https://www.ism.ac.jp/~ayaka/" target="_blank"><b>Ayaka Sakata</b></a> (The Institute of Statistical Mathematics)</br>
							</li>
							</ul>
							</div>
-->
							</section>
							<section id="program" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Program</h2>
										</header>
									</div>
								</div>
						

								<div class="table-wrapper">
									<table>
										<tbody>
											<tr>
												<td colspan="2"><h2><b>Tuesday 14th March.</b></h2></td>
											</tr>
											<tr>
												<td>09:40-09:45</td>
												<td>Opening</td>
											</tr>
											<tr>
												<td>09:45-10:45</td>
												<td>
												<div class="hidden_box">
												<p>
												<a href="https://www.gatsby.ucl.ac.uk/~gretton/" target="_blank"><b>Atrhur Gretton</b></a>  (University College London)
                                                </br>
												Title: Gradient Flows on Kernel Divergence Measures</br>
												<label for="label_1_1" style="display: inline-block; _display: inline;">Abstract</label>
												</p>
												<input type="checkbox" id="label_1_1"/>
												<div class="hidden_show">
													<h5>We construct Wasserstein gradient flows on two measures of divergence, and study their convergence properties. The first divergence measure is the Maximum Mean Discrepancy (MMD): an integral probability metric defined for a reproducing kernel Hilbert space (RKHS), which serves as a metric on probability measures for a sufficiently rich RKHS. We obtain conditions for convergence of the gradient flow towards a global optimum, and relate this flow to the problem of optimizing neural networks. The second divergence measure on which we define a flow is the KALE (KL Approximate Lower-bound Estimator) divergence. This is a regularized version of the Fenchel dual problem defining the KL over a restricted class of functions (again, a Reproducing Kernel Hilbert Space (RKHS)). We also propose a way to regularize both the MMD and KALE gradient flows, based on an injection of noise in the gradient. This algorithmic fix comes with theoretical and empirical evidence. We compare the MMD and KALE flows, illustrating that the KALE gradient flow is particularly well suited when the target distribution is supported on a low-dimensional manifold.</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>11:00-12:00</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="https://allmodelsarewrong.net" target="_blank"><b>Song Liu</b></a>  (University of Bristol)
                                                </br>
												Title: TBW</br>
												<label for="label_1_2" style="display: inline-block; _display: inline;">Abstract</label>
												</p>
												<input type="checkbox" id="label_1_2"/>
												<div class="hidden_show">
													<h5>TBW</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>12:00-13:40</td>
												<td>Lunch break</td>
											</tr>
											<tr>
												<td>13:40-14:40</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="https://sites.google.com/view/sophielanger/start" target="_blank"><b>Sophie Langer</b></a>  (University of Twente)
                                                </br>
												Title: TBW</br>
												<label for="label_1_3" style="display: inline-block; _display: inline;">Abstract</label></p>
												<input type="checkbox" id="label_1_3"/>
												<div class="hidden_show">
													<h5>TBW</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>15:05-16:05</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="http://ibis.t.u-tokyo.ac.jp/suzuki/" target="_blank"><b>Taiji Suzuki</b></a> (The University of Tokyo)
												</br>
												Title: TBW</br>
												<label for="label_1_4" style="display: inline-block; _display: inline;">Abstract</label></p>
												<input type="checkbox" id="label_1_4"/>
												<div class="hidden_show">
													<h5>TBW</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>16:20-17:20</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="https://hermite.jp" target="_blank"><b>Han Bao</b></a> (Kyoto University)
												</br>
												Title: Proper Losses, Moduli of Convexity, and Surrogate Regret Bounds
</br>
												<label for="label_1_5" style="display: inline-block; _display: inline;">Abstract</label>
                                                </p>
												<input type="checkbox" id="label_1_5"/>
												<div class="hidden_show">
													<h5>Proper losses (or proper scoring rules) have been used for over half a century to elicit users' subjective probability from the observations. In the recent machine learning community, we often tackle downstream tasks such as classification and bipartite ranking with the elicited probabilities. Here, we engage in assessing the quality of the elicited probabilities with different proper losses, which can be characterized by surrogate regret bounds to describe the convergence speed of an estimated probability to the optimal one when optimizing a proper loss. This work contributes to a sharp analysis of surrogate regret bounds in two ways. First, we provide general surrogate regret bounds for proper losses measured by the $L^1$ distance. This abstraction eschews a tailor-made analysis of each downstream task and delineates how universally a loss function operates. Our analysis relies on a classical mathematical tool known as the moduli of convexity, which is of independent interest per se. Second, we evaluate the surrogate regret bounds with polynomials to identify the quantitative convergence rate. These devices enable us to compare different losses, with which we can confirm that the lower bound of the surrogate regret bounds is $\Omega(\epsilon^{1/2})$ for popular loss functions.</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td colspan="2"><h2><b>Wednesday 15th March.</b></h2></td>
											</tr>
											<tr>
												<td>09:45-10:45</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="https://kaba-lab.org/en/" target="_blank"><b>Yoshiyuki Kabashima</b></a> (The University of Tokyo)
												</br>
												Title: Statistical mechanics approach to linear regression</br>
												<label for="label_2_1" style="display: inline-block; _display: inline;">Abstract</label></p>
												<input type="checkbox" id="label_2_1"/>
												<div class="hidden_show">
													<h5>
														We illustrate how statistical mechanics can be employed to analyze machine learning problems through its application to linear regression models. This talk is based on a collaboration with Dominik Doellerer (LMU Munich) and Takashi Takahashi (U Tokyo).
													</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>11:00-12:00</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="https://www.ism.ac.jp/~ayaka/" target="_blank"><b>Ayaka Sakata</b></a> (The Institute of Statistical Mathematics)
												</br>
												Title: Decision Theoretic Cutoff and ROC Analysis for Bayesian Optimal Group Testing</br>
												<label for="label_2_2" style="display: inline-block; _display: inline;">Abstract</label>
												</p>
												<input type="checkbox" id="label_2_2"/>
												<div class="hidden_show">
													<h5>
														In this presentation, we consider the problem of Bayesian inference of the items' states from the test results. We focus on the Bayesian optimal setting and consider the linear regime where the fraction of defective items is O(1). In this setting, we show that 'perfect reconstruction' of the items' states is impossible, and cutoff is required to distinguish between defective and non-defective items. We derive the general expression of the optimal cutoff value that minimizes the expected risk function, and evaluate the performance of Bayesian group testing without knowing the true states of the items.</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>12:00-13:40</td>
												<td>Lunch break</td>
											</tr>
											<tr>
												<td>13:40-14:40</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="https://ai.stanford.edu/~tengyuma/" target="_blank"><b>Tengyu Ma</b></a> (Stanford University)
												</br>
												Title: TBW</br>
												<label for="label_2_3" style="display: inline-block; _display: inline;">Abstract</label>
                                                </p>
												<input type="checkbox" id="label_2_3"/>
												<div class="hidden_show">
													<h5>
														TBW</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>15:05-16:05</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="https://sites.google.com/view/mimaizumi/" target="_blank"><b>Masaaki Imaizumi</b></a> (The University of Tokyo)
												</br>
												Title: TBW</br>
												<label for="label_2_4" style="display: inline-block; _display: inline;">Abstract</label></p>
												<input type="checkbox" id="label_2_4"/>
												<div class="hidden_show">
													<h5>
														TBW</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>16:20-17:20</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="https://takaosa.github.io/index.html" target="_blank"><b>Takayuki Osa</b></a>  (The University of Tokyo)
												</br>
												Title: Discovering diverse solutions in reinforcement learning</br>
												<label for="label_2_5" style="display: inline-block; _display: inline;">Abstract</label></p>
												<input type="checkbox" id="label_2_5"/>
												<div class="hidden_show">
													<h5>
														Reinforcement learning (RL) has achieved remarkable success in various applications. However, sample efficiency of training and vulnerability of a policy are typical limitations of RL. In this talk, we present our work that addresses these issues by learning diverse behaviors in RL. We demonstrate that few-shot adaptation to the change in the environment can be done by learning diverse behaviors in training. Additionally, we demonstrate that a policy in multi-agent RL can be robustified by learning diverse behaviors.</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td colspan="2"><h2><b>Wednesday 16th March.</b></h2></td>
											</tr>
											<tr>
												<td>10:00-11:00</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="https://www.microsoft.com/en-us/research/people/gregyang/" target="_blank"><b>Greg Yang</b></a> (Microsoft Research)
												</br>
												Title: The unreasonable effectiveness of mathematics in large scale deep learning</br>
												<label for="label_3_1" style="display: inline-block; _display: inline;">Abstract</label></p>
												<input type="checkbox" id="label_3_1"/>
												<div class="hidden_show">
													<h5>Recently, the theory of infinite-width neural networks led to the first technology, muTransfer, for tuning enormous neural networks that are too expensive to train more than once. For example, this allowed us to tune the 6.7 billion parameter version of GPT-3 using only 7% of its pretraining compute budget, and with some asterisks, we get a performance comparable to the original GPT-3 model with twice the parameter count. In this talk, I will explain the core insight behind this theory. In fact, this is an instance of what I call the *Optimal Scaling Thesis*, which connects infinite-size limits for general notions of “size” to the optimal design of large models in practice, illustrating a way for theory to reliably guide the future of AI. I'll end with several concrete key mathematical research questions whose resolutions will have incredible impact on how practitioners scale up their NNs.</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>11:10-12:10</td>
												<td>
											<div class="hidden_box">
												<p><a href="https://sites.google.com/view/shosonoda/home" target="_blank">
												<b>Sho Sonoda</b></a> (The University of Tokyo)
												</br>
												Title: Ridgelet Transforms for Neural Networks on Manifolds and Hilbert Spaces</br>
												<label for="label_3_2" style="display: inline-block; _display: inline;">Abstract</label>
                                                </p>
												<input type="checkbox" id="label_3_2"/>
												<div class="hidden_show">
													<h5>
														I will explain a systematic scheme to turn a depth-2 infinitely-wide fully-connected neural network into the inverse Fourier transform. As applications, we see that neural networks on manifolds and Hilbert spaces can also be turned into the Fourier transforms on manifolds and Hilbert spaces respectively.</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>12:10-13:40</td>
												<td>Lunch break</td>
											</tr>
											<tr>
												<td>13:40-14:40</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="https://sejdino.github.io" target="_blank"><b>Dino Sejdinovic</b></a> (University of Adelaide)
												</br>
												Title: Returning The Favour: When Machine Learning Benefits From Causal Models</br>
												<label for="label_3_3" style="display: inline-block; _display: inline;">Abstract</label>
                                                </p>
												<input type="checkbox" id="label_3_3"/>
												<div class="hidden_show">
													<h5>
														A directed acyclic graph (DAG) provides valuable prior knowledge that is often discarded in machine learning tasks. We show that the collider structures in DAGs provide meaningful inductive biases, which constrain the regression hypothesis space and improve predictive performance. We consider frameworks to incorporate probabilistic causal knowledge arising from a collider in a regression problem. When the hypothesis space is a reproducing kernel Hilbert space, we prove a strictly positive generalisation benefit under mild assumptions and provide closed-form estimators of the empirical risk minimiser. Experiments on synthetic and climate model data demonstrate performance gains of the proposed methodology.</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>15:05-16:05</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="https://yutomiyatake.github.io" target="_blank"><b>Yuto Miyatake</b></a>  (Osaka University)
												</br>
												Title: Adjoint-based exact Hessian computation</br>
												<label for="label_3_4" style="display: inline-block; _display: inline;">Abstract</label>
												</p>
												<input type="checkbox" id="label_3_4"/>
												<div class="hidden_show">
													<h5>In this talk, we focus on the Hessian matrix of a function that involves the numerical solution of an initial value problem, with respect to the initial data and system parameters. Such a matrix arises frequently in the adjoint method, particularly in the context of data assimilation and ODENet. In these contexts, a linear system whose coefficient matrix is the Hessian needs to be solved. The conjugate gradient (CG) method is a suitable solver for this task, but it requires a Hessian-vector multiplication. The Hessian-vector multiplication can be approximated by numerically integrating the second-order adjoint system backwardly. However, this approach may not preserve the symmetry of the Hessian matrix, which can hinder the convergence of the CG method, particularly when the numerical solutions of the original system and second-order adjoint system are not sufficiently accurate.
														<br>In this talk, we propose an algorithm that computes the Hessian-vector multiplication exactly. We achieve this by providing a concise derivation of the second-order adjoint system and by applying a particular numerical method to solve it. Specifically, we use symplectic partitioned Runge-Kutta methods. Our algorithm ensures that the Hessian matrix remains symmetric./h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>16:20-17:20</td>
												<td>
											<div class="hidden_box">
												<p>
												<a href="http://www.ism.ac.jp/~fukumizu/" target="_blank"><b>Kenji Fukumizu</b></a>  (The Institute of Statistical Mathematics)
												</br>
												Title: TBW</br>
												<label for="label_3_5" style="display: inline-block; _display: inline;">Abstract</label>
												</p>
												<input type="checkbox" id="label_3_5"/>
												<div class="hidden_show">
													<h5>
														TBW</h5>
												</div>
											</div>
											</td>
											</tr>
											<tr>
												<td>18:00-21:00</td>
												<td>Social Networking</td>
											</tr>

										</tbody>
									</table>
								</div>
							
								</section>

							<section id="organizers" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Organizers</h2>
										</header>
											<ul class="alt2">
											<li>
												Masaaki Imaizumi, The University of Tokyo
											</li>
											<li>
												Taiji Suzuki, The University of Tokyo
											</li>
											<li>
												Kenji Fukumizu, The Institute of Statistical Mathematics
											</li>
											<li>
												Tatsuya Harada, The University of Tokyo
											</li>
											</ul>
									</div>
								</div>	
									
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Sponsors</h2>
										</header>
											<p>This workshop is supported by the following institution and grant:
											<li>
												<a href="http://www.ism.ac.jp/noe/sml-center/en/index.html" target="_blank">Research Center for Statistical Machine Learning, The Institute of Statistical Mathematics</a>
											</li>
											<li>
												<a href="https://www.nedo.go.jp/english/index.html" target="_blank">Japan Science and Technology Agency, CREST</a>
											</li>
												<ul style="margin: 0 0 0 0;list-style-type: none;"><li style="line-height:20px;"><span style="font-size : smaller;">
												"Innovation of Deep Structured Models with Representation of Mathematical Intelligence" 
												in 
												"Creating information utilization platform by integrating mathematical and information sciences, and development to society"</span></li></ul>
											<li>
												<a href="https://data-descriptive-science.org" target="_blank">Grant-in-Aid for Transformative Research Areas (A)</a>
											</li>
												<ul style="margin: 0 0 0 0;list-style-type: none;"><li style="line-height:20px;"><span style="font-size : smaller;">
												"Establishing data descriptive science and its cross-disciplinary applications" 
											</ul>
											</p>
									</div>
								</div>
							</section>
					

							<section id="location" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Location</h2>
										</header>		

										<table>
										<tr>
										<td width="50%">
										
										<img src="images/build.jpg" width="100%" align="middle">
										</td>
										<td>
										<p>Address: The Institute of Statistical Mathematics</br>
										10-3 Midori-cho, Tachikawa, Tokyo 190-8562, Japan.</br>
										For detail, see <a href="http://www.ism.ac.jp/access/index_e.html" target="_blank">official access information</a>.</p>	
											
										</td>
										</tr>
										</table>
										
										<iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d25916.50711608945!2d139.39133387198493!3d35.71236043577579!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x6018e10973c7adc1%3A0x9bad210c909fdb3e!2sThe+Institute+of+Statistical+Mathematics!5e0!3m2!1sen!2sjp!4v1506870290564" width="100%" height="300em" frameborder="0" style="border:0" allowfullscreen></iframe>
									</div>
								</div>
							</section>


					</div>

				<!-- Footer
					<footer id="footer">
						<p class="copyright">Copyright &copy; At?l?m Gune? Baydin. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
					</footer>  -->
			</div>

		<!-- Scripts  -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>
	</body>
</html>

